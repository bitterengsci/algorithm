
<!-- TOC -->

- [1. MLåŸºç¡€æ¦‚å¿µç±»](#1-mlåŸºç¡€æ¦‚å¿µç±»)
  - [1.1. Reguarlization](#11-reguarlization)
  - [1.2. Metric](#12-metric)
  - [1.3. Loss & Optimization](#13-loss--optimization)
- [2. DLåŸºç¡€æ¦‚å¿µç±»](#2-dlåŸºç¡€æ¦‚å¿µç±»)
- [3. MLæ¨¡å‹ç±»](#3-mlæ¨¡å‹ç±»)
  - [3.1. Regression:](#31-regression)
- [4. Convolution](#4-convolution)
  - [4.1. Clustering and EM:](#41-clustering-and-em)
  - [4.2. Decision Tree](#42-decision-tree)
  - [4.3. Ensemble Learning](#43-ensemble-learning)
  - [4.4. Generative Model](#44-generative-model)
  - [4.5. Logistic Regression](#45-logistic-regression)
  - [4.6. å…¶ä»–æ¨¡å‹](#46-å…¶ä»–æ¨¡å‹)
- [5. Data Processing](#5-data-processing)
- [6. implementation & derivation](#6-implementation--derivation)
- [7. NLP/RNNç›¸å…³](#7-nlprnnç›¸å…³)
- [8. CNN/CVç›¸å…³](#8-cnncvç›¸å…³)
- [9. VAE, GANs](#9-vae-gans)
- [10. Loss Function & Optimization](#10-loss-function--optimization)
- [11. Detection](#11-detection)
- [12. é¡¹ç›®ç»éªŒç±»](#12-é¡¹ç›®ç»éªŒç±»)
- [13. å…³äºå‡†å¤‡è€ƒML æ¦‚å¿µçš„é¢è¯•çš„ä¸€äº›å»ºè®®](#13-å…³äºå‡†å¤‡è€ƒml-æ¦‚å¿µçš„é¢è¯•çš„ä¸€äº›å»ºè®®)
- [14. Pytorch Example](#14-pytorch-example)
- [ToDo List](#todo-list)

<!-- /TOC -->

# 1. MLåŸºç¡€æ¦‚å¿µç±»
overfitting/underfiting:
- underfitting means large training error, large generalization error; overfitting means small training error, large generalization error
- overfitting means training loss is small and testing loss is large, small regularization
- overfit means complex model, underfit means over-simplified model.
- overfitting: model is tailored to a particular dataset and is unable to generalise to other datasets

bias/variance trade off
- A set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of parameter estimates across samples, and vice versa.
- linear/logistic regression: underfittingâ€”high bias, overfittingâ€”high variance
- high bias, low variance (underfitting: model too simple, model miss relevant relations between features and target output)
- high variance, low bias (overfitting: model modelled random noise in the training data)

Bias Variance Decomposition: Error = Bias ** 2 + Variance + Irreducible Error


How to overcome/prevent Overfitting:
1. Regularization 
  - L1 regularization (lasso penalty) favours few non-zero coefficients   Î»âˆ‘âˆ£Î¸âˆ£
    - L1 also performs vairable selection and yield sparse models.
  - L2 regularization (ridge/Tikhonov penalty) favours small coefficients	 Î»âˆ‘Î¸2
    - L2 more sensitive to outliers; gradient exploding issue
  - Mixed L1/L2 regularization (elastic net)  Î»1âˆ‘âˆ£Î¸âˆ£+Î»2âˆ‘Î¸2
  - L^p regularization (penalty on parameters)
2. early stopping: interrupt training when its performance on the validation set starts dropping
3. Max-norm: for each neuron the weight Î¸ of the incoming connections are constrained such that  ğ„Î¸ğ„2â‰¤r  (clipping)
  - compute ğ„Î¸ğ„2 after each training step and clip Î¸ if needed Î¸=Î¸r/ğ„Î¸ğ„2
  - max-norm hyperparameter r increases the amount of regularization
4. dropout: at every training step, every neuron (input/hidden) has a probability of p of being temporarily dropped out
   â‰ˆ an ensemble learning method
5. data augmentation: variation of input data; artificially boosting the size of training set
 by rotate, shift(translate), resize(scale), flip(reflect), or add different lighting conditions or noise that can be learnt
6. Stochastic Gradient Descent

Generative/Discrimitive:
- Generative models model the (actual) distribution of each class.
- given training data, Generative models generate new samples from same distribution, learn Pmodel(x)â‰ˆPdata(x)
- Discriminative models learn the (hard or soft) decision boundary between the classes. 

Give a set of ground truths and 2 models, how do you be confident that one model is better than another?

## 1.1. Reguarlization
Occamâ€™s Razor: among competing hypotheses, the simplest is the best. 

L1 regularization (lasso penalty) 
- favours few non-zero coefficients   Î»âˆ‘âˆ£Î¸âˆ£
- prior: Laplace distribution with mean zero and a scale parameter a function of Î»

L2 regularization (ridge/Tikhonov penalty) 
- favours small coefficients	 Î»âˆ‘Î¸2
- prior: a Gaussian with mean zero and standard deviation a function of Î»
- Optimizing with respect to L2 norm corresponds to maximizing the (log-)likelihood of the data under a Gaussian prior. 

Lasso/Ridgeçš„æ¨å¯¼

Why L1 sparse: L2 penalties in some sense discourage sparsity by yielding diminishing returns as elements are moved closer to zero

ä¸ºä»€ä¹ˆregularization works
ä¸ºä»€ä¹ˆregularizationç”¨L1 L2ï¼Œè€Œä¸æ˜¯L3, L4..


## 1.2. Metric
Confusion Matrix
- A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned)

Precision (Positive Predictive Value) = true positive / (true positive + false positive)
- measure of exactness
Recall (True Positive rate, sensitivity) = true positive / (true positive + false negative)
- measure of completeness

F/F1-Score: weighted average of precision and recall 2 * (P * R) / (P + R)
- consider both FP and FN

True Negative Rate/Specificity = TN / (FP + TN)
False Positive Rate = FP / (FP + TN)
Accuracy

ROC curve (receiver operating characteristic)
- showing the performance of a classification model at all classification thresholds
- x-axis: FP rate, y-axis: TP rate  (TPR vs. FPR at different classification thresholds)
- often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives). --> precision and recall trade-off
- The curves of different models can be compared directly in general or for different thresholds.
- The area under the curve (AUC) can be used as a summary of the model skill

Precision-Recall Curve
- a plot of precision (y-axis) and recall (x-axis) for different thresholds

AUC (Area Under the ROC Curve)
- an aggregate measure of performance across all possible classification thresholds
- A model whose predictions are 100% wrong has an AUC of 0; one whose predictions are 100% correct has an AUC of 1.
- scale invariant. It measures how well predictions are ranked, rather than their absolute values.
- classification-threshold invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.

Type I error is a false positive, Type II error is a false negative. 
- Type I error means claiming something has happened when it hasnâ€™t
- Type II error means that you claim nothing is happening when in fact something is. 
- e.g., Type I error as telling a man he is pregnant, Type II error as you tell a pregnant woman she isnâ€™t carrying a baby.


label ä¸å¹³è¡¡æ—¶ç”¨ä»€ä¹ˆmetric
åˆ†ç±»é—®é¢˜è¯¥é€‰ç”¨ä»€ä¹ˆmetricï¼Œand why
confusion matrix
AUCçš„è§£é‡Š (the probability of ranking a randomly selected positive sample higher blablabla....)
Log-lossæ˜¯ä»€ä¹ˆï¼Œä»€ä¹ˆæ—¶å€™ç”¨logloss
è¿˜æœ‰ä¸€äº›å’Œåœºæ™¯æ¯”è¾ƒç›¸å…³çš„é—®é¢˜ï¼Œæ¯”å¦‚ranking designçš„æ—¶å€™ç”¨ä»€ä¹ˆmetricï¼Œæ¨èçš„æ—¶å€™ç”¨ä»€ä¹ˆ

stratified cross-validation
- split preserves the ratio of the categories on both the training and validation datasets
- applications: imbalanced dataset


## 1.3. Loss & Optimization
ç”¨MSEåšlossçš„Logistic Rregressionæ˜¯convex problemå—
è§£é‡Šå¹¶å†™å‡ºMSEçš„å…¬å¼, ä»€ä¹ˆæ—¶å€™ç”¨åˆ°MSE?
Linear Regressionæœ€å°äºŒä¹˜æ³•å’ŒMLEå…³ç³»
ä»€ä¹ˆæ˜¯relative entropy/crossentropy

Kullback-Leiber Divergence
- intuition: 
- a measure of difference of 2 probability distributions (how close 2 distributions are)


Logistic Regressionçš„lossæ˜¯ä»€ä¹ˆ
Logistic Regressionçš„ Loss æ¨å¯¼
SVMçš„lossæ˜¯ä»€ä¹ˆ
Multiclass Logistic Regressionç„¶åé—®äº†ä¸€ä¸ªä¸ºä»€ä¹ˆç”¨cross entropyåšcost function
Decision Tree split nodeçš„æ—¶å€™ä¼˜åŒ–ç›®æ ‡æ˜¯å•¥


# 2. DLåŸºç¡€æ¦‚å¿µç±»

bias term in DNN: shift the activation function

DNNä¸ºä»€ä¹ˆè¦æœ‰bias term, bias termçš„intuitionæ˜¯ä»€ä¹ˆ
ä»€ä¹ˆæ˜¯Back Propagation

Epoch
- Epoch: one pass through the whole training set
- Batch: examples processed together in one pass (forward and backward)
- Iteration: number of training examples / Batch size
- If there are N training samples and use B batch size, then it takes N/B iterations to complete on epoch

Gradient Vanishing & Exploding
- Vanishing:
    - activation function squishes a large input space into a small one (e.g., sigmoid 0 to 1); large change in the input results in small change in the output --> derivative small
    - use other activations, like ReLU (not cause a small derivative)
    - residual network
    - batch normalization (normalize the input) 
        - if batchsize = 1?
        - multiple publications have shown BN performance degrade for batch_size under 32, and severely for <= 8.
    - use more complex RNN
- Exploding: gradient becomes very large, preventing the algorithm from converging
    - gradient clipping (clip by value, clip by norm)
    - use LSTM

Initialize all weights to be 0?
- constant initialization leads to same gradient propagated back everywhere (symmetric)

DNNå’ŒLogistic Regressionçš„åŒºåˆ«; ä¸ºä»€ä¹ˆDNNçš„æ‹Ÿåˆèƒ½åŠ›æ¯”Logistic Regressionå¼º? 
- Non-linearity introduced by activation functions

Hyperparameter tuning in DL: random search, grid search

prevent overfitting in DL: dropout, early stopping, data augumentation
- ä»€ä¹ˆæ˜¯Dropoutï¼Œwhy it worksï¼Œdropoutçš„æµç¨‹æ˜¯ä»€ä¹ˆ (è®­ç»ƒå’Œæµ‹è¯•æ—¶çš„åŒºåˆ«)
- ä»€ä¹ˆæ˜¯Batch Norm, why it works, BNçš„æµç¨‹æ˜¯ä»€ä¹ˆ (è®­ç»ƒå’Œæµ‹è¯•æ—¶çš„åŒºåˆ«)

Activation Functions
1. Logistic/Sigmoid: 1 / (1 + exp(-x))
    - 0.0 ~ 1.0, f(z) saturates at 0 or 1, i.e., derivative becomes almost 0/megligible.
    - gradient vanish problem, time-consuming to compute exponentials, not symmetric w.r.t. the origin
2. Hyperbolic Tangent tanh: (e^x â€“ e^-x) / (e^x + e^-x)
    - -1.0 ~ +1.0, f(z) saturates at -1 or 1
    - gradient vanish problem
    - faster than sigmoid, symmetric w.r.t. the origin
3. Rectified Linear Unit ReLU
    - discontinuity at 0 but has sub-gradient
    - outperform the sigmoid function
    - gradient will not vanish when z is very large, but gradient becomes 0 when z becomes negative, which may still lead to vanishing gradient problem (partially solve vanishing problem); neuron dies in negative
    - leaky relu: f(z)=0.01z, z<0; z, z>0
    - Parametric ReLU    f(z)=az, z<0; z, z>0    (learning parameter a in training)
    - Exponential Linear unit  f(z)=a(e^zï¹£1), z<0; z, zâ‰¥0
4. maxout: max(w1x+b1, w2x+b2)   a piecewise-linear function

Softmax: e^x / sum(e^x)


ä¸ºä»€ä¹ˆéœ€è¦non-linear activation functions?
- å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼Œç½‘ç»œçš„æ¯ä¸€å±‚ç›¸å½“äºf(wx+b)=f(w'x)ï¼Œå¯¹äºçº¿æ€§å‡½æ•°ï¼Œå…¶å®ç›¸å½“äºf(x)=xï¼Œé‚£ä¹ˆåœ¨çº¿æ€§æ¿€æ´»å‡½æ•°ä¸‹ï¼Œæ¯ä¸€å±‚ç›¸å½“äºç”¨ä¸€ä¸ªçŸ©é˜µå»ä¹˜ä»¥xï¼Œé‚£ä¹ˆå¤šå±‚å°±æ˜¯åå¤çš„ç”¨çŸ©é˜µå»ä¹˜ä»¥è¾“å…¥ã€‚æ ¹æ®çŸ©é˜µçš„ä¹˜æ³•æ³•åˆ™ï¼Œå¤šä¸ªçŸ©é˜µç›¸ä¹˜å¾—åˆ°ä¸€ä¸ªå¤§çŸ©é˜µã€‚æ‰€ä»¥çº¿æ€§æ¿€åŠ±å‡½æ•°ä¸‹ï¼Œå¤šå±‚ç½‘ç»œä¸ä¸€å±‚ç½‘ç»œç›¸å½“ã€‚æ¯”å¦‚ï¼Œä¸¤å±‚çš„ç½‘ç»œf(W1*f(W2x))=W1W2x=Wxã€‚
- éçº¿æ€§å˜æ¢æ˜¯æ·±åº¦å­¦ä¹ æœ‰æ•ˆçš„åŸå› ä¹‹ä¸€ã€‚åŸå› åœ¨äºéçº¿æ€§ç›¸å½“äºå¯¹ç©ºé—´è¿›è¡Œå˜æ¢ï¼Œå˜æ¢å®Œæˆåç›¸å½“äºå¯¹é—®é¢˜ç©ºé—´è¿›è¡Œç®€åŒ–ï¼ŒåŸæ¥çº¿æ€§ä¸å¯è§£çš„é—®é¢˜ç°åœ¨å˜å¾—å¯ä»¥è§£äº†ã€‚

ç¥ç»ç½‘ç»œä¸­æ¿€æ´»å‡½æ•°çš„çœŸæ­£æ„ä¹‰ï¼Ÿä¸€ä¸ªæ¿€æ´»å‡½æ•°éœ€è¦å…·æœ‰å“ªäº›å¿…è¦çš„å±æ€§ï¼Ÿè¿˜æœ‰å“ªäº›å±æ€§æ˜¯å¥½çš„å±æ€§ä½†ä¸å¿…è¦çš„ï¼Ÿ
- (1) éçº¿æ€§ï¼šå³å¯¼æ•°ä¸æ˜¯å¸¸æ•°ã€‚è¿™ä¸ªæ¡ä»¶æ˜¯å¤šå±‚ç¥ç»ç½‘ç»œçš„åŸºç¡€ï¼Œä¿è¯å¤šå±‚ç½‘ç»œä¸é€€åŒ–æˆå•å±‚çº¿æ€§ç½‘ç»œã€‚è¿™ä¹Ÿæ˜¯æ¿€æ´»å‡½æ•°çš„æ„ä¹‰æ‰€åœ¨ã€‚
- (2) å‡ ä¹å¤„å¤„å¯å¾®ï¼šå¯å¾®æ€§ä¿è¯äº†åœ¨ä¼˜åŒ–ä¸­æ¢¯åº¦çš„å¯è®¡ç®—æ€§ã€‚ä¼ ç»Ÿçš„æ¿€æ´»å‡½æ•°å¦‚sigmoidç­‰æ»¡è¶³å¤„å¤„å¯å¾®ã€‚å¯¹äºåˆ†æ®µçº¿æ€§å‡½æ•°æ¯”å¦‚ReLUï¼Œåªæ»¡è¶³å‡ ä¹å¤„å¤„å¯å¾®ï¼ˆå³ä»…åœ¨æœ‰é™ä¸ªç‚¹å¤„ä¸å¯å¾®ï¼‰ã€‚å¯¹äºSGDç®—æ³•æ¥è¯´ï¼Œç”±äºå‡ ä¹ä¸å¯èƒ½æ”¶æ•›åˆ°æ¢¯åº¦æ¥è¿‘é›¶çš„ä½ç½®ï¼Œæœ‰é™çš„ä¸å¯å¾®ç‚¹å¯¹äºä¼˜åŒ–ç»“æœä¸ä¼šæœ‰å¾ˆå¤§å½±å“[1]ã€‚
- (3) è®¡ç®—ç®€å•ï¼šéçº¿æ€§å‡½æ•°æœ‰å¾ˆå¤šã€‚æç«¯çš„è¯´ï¼Œä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œä¹Ÿå¯ä»¥ä½œä¸ºä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œç±»ä¼¼äºNetwork In Network[2]ä¸­æŠŠå®ƒå½“åšå·ç§¯æ“ä½œçš„åšæ³•ã€‚ä½†æ¿€æ´»å‡½æ•°åœ¨ç¥ç»ç½‘ç»œå‰å‘çš„è®¡ç®—æ¬¡æ•°ä¸ç¥ç»å…ƒçš„ä¸ªæ•°æˆæ­£æ¯”ï¼Œå› æ­¤ç®€å•çš„éçº¿æ€§å‡½æ•°è‡ªç„¶æ›´é€‚åˆç”¨ä½œæ¿€æ´»å‡½æ•°ã€‚è¿™ä¹Ÿæ˜¯ReLUä¹‹æµæ¯”å…¶å®ƒä½¿ç”¨Expç­‰æ“ä½œçš„æ¿€æ´»å‡½æ•°æ›´å—æ¬¢è¿çš„å…¶ä¸­ä¸€ä¸ªåŸå› ã€‚
- (4) éé¥±å’Œæ€§ï¼ˆsaturationï¼‰ï¼šé¥±å’ŒæŒ‡çš„æ˜¯åœ¨æŸäº›åŒºé—´æ¢¯åº¦æ¥è¿‘äºé›¶ï¼ˆå³æ¢¯åº¦æ¶ˆå¤±ï¼‰ï¼Œä½¿å¾—å‚æ•°æ— æ³•ç»§ç»­æ›´æ–°çš„é—®é¢˜ã€‚æœ€ç»å…¸çš„ä¾‹å­æ˜¯Sigmoidï¼Œå®ƒçš„å¯¼æ•°åœ¨xä¸ºæ¯”è¾ƒå¤§çš„æ­£å€¼å’Œæ¯”è¾ƒå°çš„è´Ÿå€¼æ—¶éƒ½ä¼šæ¥è¿‘äº0ã€‚æ›´æç«¯çš„ä¾‹å­æ˜¯é˜¶è·ƒå‡½æ•°ï¼Œç”±äºå®ƒåœ¨å‡ ä¹æ‰€æœ‰ä½ç½®çš„æ¢¯åº¦éƒ½ä¸º0ï¼Œå› æ­¤å¤„å¤„é¥±å’Œï¼Œæ— æ³•ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚ReLUåœ¨x>0æ—¶å¯¼æ•°æ’ä¸º1ï¼Œå› æ­¤å¯¹äºå†å¤§çš„æ­£å€¼ä¹Ÿä¸ä¼šé¥±å’Œã€‚ä½†åŒæ—¶å¯¹äºx<0ï¼Œå…¶æ¢¯åº¦æ’ä¸º0ï¼Œè¿™æ—¶å€™å®ƒä¹Ÿä¼šå‡ºç°é¥±å’Œçš„ç°è±¡ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹é€šå¸¸ç§°ä¸ºdying ReLUï¼‰ã€‚Leaky ReLUå’ŒPReLUçš„æå‡ºæ­£æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ã€‚
- (5) å•è°ƒæ€§ï¼ˆmonotonicï¼‰ï¼šå³å¯¼æ•°ç¬¦å·ä¸å˜ã€‚è¿™ä¸ªæ€§è´¨å¤§éƒ¨åˆ†æ¿€æ´»å‡½æ•°éƒ½æœ‰ï¼Œé™¤äº†è¯¸å¦‚sinã€cosç­‰ã€‚ä¸ªäººç†è§£ï¼Œå•è°ƒæ€§ä½¿å¾—åœ¨æ¿€æ´»å‡½æ•°å¤„çš„æ¢¯åº¦æ–¹å‘ä¸ä¼šç»å¸¸æ”¹å˜ï¼Œä»è€Œè®©è®­ç»ƒæ›´å®¹æ˜“æ”¶æ•›ã€‚
- (6) è¾“å‡ºèŒƒå›´æœ‰é™ï¼šæœ‰é™çš„è¾“å‡ºèŒƒå›´ä½¿å¾—ç½‘ç»œå¯¹äºä¸€äº›æ¯”è¾ƒå¤§çš„è¾“å…¥ä¹Ÿä¼šæ¯”è¾ƒç¨³å®šï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæ—©æœŸçš„æ¿€æ´»å‡½æ•°éƒ½ä»¥æ­¤ç±»å‡½æ•°ä¸ºä¸»ï¼Œå¦‚Sigmoidã€TanHã€‚ä½†è¿™å¯¼è‡´äº†å‰é¢æåˆ°çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè€Œä¸”å¼ºè¡Œè®©æ¯ä¸€å±‚çš„è¾“å‡ºé™åˆ¶åˆ°å›ºå®šèŒƒå›´ä¼šé™åˆ¶å…¶è¡¨è¾¾èƒ½åŠ›ã€‚å› æ­¤ç°åœ¨è¿™ç±»å‡½æ•°ä»…ç”¨äºæŸäº›éœ€è¦ç‰¹å®šè¾“å‡ºèŒƒå›´çš„åœºåˆï¼Œæ¯”å¦‚æ¦‚ç‡è¾“å‡ºï¼ˆæ­¤æ—¶losså‡½æ•°ä¸­çš„logæ“ä½œèƒ½å¤ŸæŠµæ¶ˆå…¶æ¢¯åº¦æ¶ˆå¤±çš„å½±å“ï¼‰ã€LSTMé‡Œçš„gateå‡½æ•°ã€‚
- (7) æ¥è¿‘æ’ç­‰å˜æ¢ï¼ˆidentityï¼‰ï¼šå³çº¦ç­‰äºxã€‚è¿™æ ·çš„å¥½å¤„æ˜¯ä½¿å¾—è¾“å‡ºçš„å¹…å€¼ä¸ä¼šéšç€æ·±åº¦çš„å¢åŠ è€Œå‘ç”Ÿæ˜¾è‘—çš„å¢åŠ ï¼Œä»è€Œä½¿ç½‘ç»œæ›´ä¸ºç¨³å®šï¼ŒåŒæ—¶æ¢¯åº¦ä¹Ÿèƒ½å¤Ÿæ›´å®¹æ˜“åœ°å›ä¼ ã€‚è¿™ä¸ªä¸éçº¿æ€§æ˜¯æœ‰ç‚¹çŸ›ç›¾çš„ï¼Œå› æ­¤æ¿€æ´»å‡½æ•°åŸºæœ¬åªæ˜¯éƒ¨åˆ†æ»¡è¶³è¿™ä¸ªæ¡ä»¶ï¼Œæ¯”å¦‚TanHåªåœ¨åŸç‚¹é™„è¿‘æœ‰çº¿æ€§åŒºï¼ˆåœ¨åŸç‚¹ä¸º0ä¸”åœ¨åŸç‚¹çš„å¯¼æ•°ä¸º1ï¼‰ï¼Œè€ŒReLUåªåœ¨x>0æ—¶ä¸ºçº¿æ€§ã€‚è¿™ä¸ªæ€§è´¨ä¹Ÿè®©åˆå§‹åŒ–å‚æ•°èŒƒå›´çš„æ¨å¯¼æ›´ä¸ºç®€å•ã€‚é¢å¤–æä¸€å¥ï¼Œè¿™ç§æ’ç­‰å˜æ¢çš„æ€§è´¨ä¹Ÿè¢«å…¶ä»–ä¸€äº›ç½‘ç»œç»“æ„è®¾è®¡æ‰€å€Ÿé‰´ï¼Œæ¯”å¦‚CNNä¸­çš„ResNetå’ŒRNNä¸­çš„LSTMã€‚
- (8) å‚æ•°å°‘ï¼šå¤§éƒ¨åˆ†æ¿€æ´»å‡½æ•°éƒ½æ˜¯æ²¡æœ‰å‚æ•°çš„ã€‚åƒPReLUå¸¦å•ä¸ªå‚æ•°ä¼šç•¥å¾®å¢åŠ ç½‘ç»œçš„å¤§å°ã€‚è¿˜æœ‰ä¸€ä¸ªä¾‹å¤–æ˜¯Maxoutï¼Œå°½ç®¡æœ¬èº«æ²¡æœ‰å‚æ•°ï¼Œä½†åœ¨åŒæ ·è¾“å‡ºé€šé“æ•°ä¸‹kè·¯Maxoutéœ€è¦çš„è¾“å…¥é€šé“æ•°æ˜¯å…¶å®ƒå‡½æ•°çš„kå€ï¼Œè¿™æ„å‘³ç€ç¥ç»å…ƒæ•°ç›®ä¹Ÿéœ€è¦å˜ä¸ºkå€ï¼›ä½†å¦‚æœä¸è€ƒè™‘ç»´æŒè¾“å‡ºé€šé“æ•°çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¿€æ´»å‡½æ•°åˆèƒ½å°†å‚æ•°ä¸ªæ•°å‡å°‘ä¸ºåŸæ¥çš„kå€ã€‚
- (9) å½’ä¸€åŒ–ï¼ˆnormalizationï¼‰ï¼šè¿™ä¸ªæ˜¯æœ€è¿‘æ‰å‡ºæ¥çš„æ¦‚å¿µï¼Œå¯¹åº”çš„æ¿€æ´»å‡½æ•°æ˜¯SELUï¼Œä¸»è¦æ€æƒ³æ˜¯ä½¿æ ·æœ¬åˆ†å¸ƒè‡ªåŠ¨å½’ä¸€åŒ–åˆ°é›¶å‡å€¼ã€å•ä½æ–¹å·®çš„åˆ†å¸ƒï¼Œä»è€Œç¨³å®šè®­ç»ƒã€‚åœ¨è¿™ä¹‹å‰ï¼Œè¿™ç§å½’ä¸€åŒ–çš„æ€æƒ³ä¹Ÿè¢«ç”¨äºç½‘ç»œç»“æ„çš„è®¾è®¡ï¼Œæ¯”å¦‚Batch Normalizationã€‚


Different optimizers (SGD, RMSprop, Momentum, Adagradï¼ŒAdam) çš„åŒºåˆ«

Batch vs SGD
- Stochastic gradient descent: approximate the gradient using current sample
- Mini-Batch: use a small batch of b samples
- Small batch size:
    - stalls at less accurate result, lower iteration cost
    - gradient may become sensitive to a single training sample
- Large batch size:
    - stalls at more accurate result, higher iteration cost
    - computation will become expensive and use more memory on GPU.

learning rate/step size
1. small step size: 
    - slower initial convergence, stalls at more accurate result
    - optimization may be slow
2. large step size: 
    - faster initial convergence, stalls at less accurate result
    - With a high learning rate, the optimization is faster but may bounce chaotically and not arrive at good local minima
	- With a very high learning rate, the optimization may diverge

Problem of Plateau, saddle point

When transfer learning makes sense?
- Transfer learning only works in deep learning if the model features learned from the first task are general.

Normalization
- batch normalization
- layer normalization
- instance normalization
- group normalization


# 3. MLæ¨¡å‹ç±»

## 3.1. Regression:
four principal assumptions for Linear Regression:
1. Linearity and additivity
2. statistical independence
3. homoscedasticity
4. normality

what will happen when we have correlated variables, how to solve
explain regression coefficient

what is the relationship between minimizing squared errorÂ and maximizing the likelihood

How could you minimize the inter-correlation between variables with Linear Regression?
if the relationship between y and x is no linear, can linear regression solve that

why use interaction variables

# 4. Convolution
Convolutional Block
- convolute (inner product) a filter on image in sliding window

CNN on images
- preserve, encode the spatial information
- translation-invariant (sliding window)

Receptive Field

Maxpooling: reduce computations by reducing the size of featuremap after pooling

Residual Network

Batch Normalization
- normalize the inputs of each layer
- BNä¿è¯æ¯ä¸€å±‚çš„è¾“å…¥åˆ†å¸ƒç¨³å®šï¼Œå¯ä»¥ä½¿å¾—è®­ç»ƒåŠ é€Ÿï¼Œä¹Ÿå¯ä»¥å¸®åŠ©å‡å°‘æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„ç°è±¡

Why small kernels are preferred?
- several small kernels than few larger ones to get the same receptive field and capture more spatial context, but with less parameters and computations
- more smaller kernels, more activations for more discriminative mapping being learned

çŸ©é˜µä¹˜æ³•AxB, Açš„å°ºå¯¸ä¸ºmxn, Bçš„å°ºå¯¸ä¸ºnxp, AxBçš„æ¯ä¸ªå…ƒç´ éœ€è¦è¿›è¡Œnæ¬¡ä¹˜æ³•å’Œn-1æ¬¡åŠ æ³•

åŸºäºäºŒæ¬¡å‡†åˆ™å‡½æ•°çš„H-Kç®—æ³•


## 4.1. Clustering and EM:
K-means clustering (explain the algorithm in detail; whether it will converge, æ”¶æ•›åˆ°global or local optimums;Â Â how to stop)

EMç®—æ³•æ˜¯ä»€ä¹ˆ
GMMæ˜¯ä»€ä¹ˆï¼Œå’ŒKmeansçš„å…³ç³»


## 4.2. Decision Tree
How regression/classification DT split nodes?
How to prevent overfitting in DT?
How to do regularization in DT?


## 4.3. Ensemble Learning
Bagging: (Bootstrap Aggregating): reduce model variance through averaging
* build weak learners in parallel
  - Bootstraping (train separate weak learners on each Bootstrap sample)
  - Aggregating results: classification uses majority votes; regression used averaging.
- å’Œç¥ç»ç½‘ç»œä¸­dropoutæ•ˆæœç±»ä¼¼


Decision Stump
* Trees that only contain one decision for classification


Random Forest: homogenous ensemble learning of decision trees
* built using Bagging (each decision tree as a parallel estimator)
- pros: a whitebox model (easy to understand, interpretable, visualizable); not require normalization; 
- cons: need uncorrelated decision trees (Bootstrapping plays a key role in creating uncorrelated decision trees.)
will random forest help reduce bias or variance/why random forest can help reduce variance?


Boosting:
* build weak learners in serial/sequential to create a strong learner
- but adaptively re-weight training data prior training each new weak learner, in order to get a higher weight to previously misclassified examples


(discrete) Adaptive Boosting (AdaBoost)
* boosting the performance of decision trees for binary classification
* Algorithm:
    - (1) Each instance in the training dataset is weighted initially as w(i) = 1/N
    - (2) For m = 1 to M
        - Fit a classfier Gm(x) to the training data using weights w(i)
        - Compute error errorm = Î£ w(i) * I(yi â‰  Gm(xi)) / Î£ w(i) 
        - Compute Î±m = log((1 - errorm) / errorm)
        - Update w(i) = w(i) * exp(Î±m * I(yi â‰  Gm(xi)))
    - (3) Final prediction sign(Î£ Î±m * Gm(x))
- pros: fast, simple; robust to overfit; can be used with text or numeric data
- cons: sensitive to noise and outlier; weak classifiers being too weak may lead to low margins and overfitting
```python
def compute_error(y, y_pred, w_i):
    '''
    Calculate the error rate of a weak classifier m. Arguments:
    y: actual target value
    y_pred: predicted value by weak classifier
    w_i: individual weights for each observation
    '''
    return (sum(w_i * (np.not_equal(y, y_pred)).astype(int))) / sum(w_i)

def compute_alpha(error):
    '''
    Calculate the weight of a weak classifier m in the majority vote of the final classifier. This is called
    alpha in chapter 10.1 of The Elements of Statistical Learning. Arguments:
    error: error rate from weak classifier m
    '''
    return np.log((1 - error) / error)

def update_weights(w_i, alpha, y, y_pred):
    ''' 
    Update individual weights w_i after a boosting iteration. Arguments:
    w_i: individual weights for each observation
    y: actual target value
    y_pred: predicted value by weak classifier  
    alpha: weight of weak classifier used to estimate y_pred
    '''  
    return w_i * np.exp(alpha * (np.not_equal(y, y_pred)).astype(int))
```


Gradient Boosted Decision Trees (GBDT):
  - Also known as gradient boosting, multiple additive regression trees, stochastic gradient boosting, gradient boosting machines
* New models are created to predict the residuals or errors of prior models and then added together to make the final prediction
- Built using Boosting (decision trees connected in series) to minimize the error of previous tree
- Decision trees in GBDT are not fit to the entire dataset. Each tree fits to the residuals from the previous one. As a result, the overall accuracy and robustness of the model gradually increase.
- Support both regression and classification predictive modeling problems
- pros: do not need bootstrapping, no correlated trees; no need to create subsamples from the dataset


XGBoost
* a library implemented GBDT and engineered for efficiency of compute time and memory resources
    - Execution Speed + Model Performance
* Three main forms of gradient boosting are supported:
    - (1) Gradient Boosting algorithm also called gradient boosting machine including the learning rate
    - (2) Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels
    - (3) Regularized Gradient Boosting with both L1 and L2 regularization
* system feature
    - Parallelization of tree construction using all of your CPU cores during training
    - Distributed Computing for training very large models using a cluster of machines
    - Out-of-Core Computing for very large datasets that donâ€™t fit into memory
    - Cache Optimization of data structures and algorithm to make best use of hardware
* Algorithm Features
    - Sparse Aware implementation with automatic handling of missing data values
    - Block Structure to support the parallelization of tree construction
    - Continued Training so that you can further boost an already fitted model on new data


Light GBM


## 4.4. Generative Model
å’ŒDiscrimitiveæ¨¡å‹æ¯”èµ·æ¥ï¼ŒGenerative æ›´å®¹æ˜“overfittingè¿˜æ˜¯underfitting


NaÃ¯ve Bayesçš„åŸç†ï¼ŒåŸºç¡€å‡è®¾æ˜¯ä»€ä¹ˆ


Linear/normal discriminant analysis (LDA) vs Quadratic discriminant analysis (QDA)
- conditional probability density function P(x|y=0) and P(x|y=1) are normal distributions N(Î¼0, Î£0) and N(Î¼1, Î£1)
- Bayes optimal solution is to predict points as being from the second class if the log of the likelihood ratios is bigger than some threshold T: (x - Î¼0)Î£0(x - Î¼0) + ln|Î£0| - (x - Î¼1)Î£1(x - Î¼1) - ln|Î£1| > T  (QDA)
- LDA: assume Î£0=Î£1, dot(w, x) > c where w = pow(Î£, -1) * (Î¼1 - Î¼0) and c = dot(w, (Î¼1 + Î¼0) / 2)
    - the criterion of an input x being in a class y is purely a function of the linear combination of the known observations.
- LDA is supervised, PCA is unsupervised; both are linear transformations; PCA finds the directions of maximal variance, LDA finds a feature subspace that maximizes class separability.


## 4.5. Logistic Regression
logistic regression vs svmï¼ˆæˆ‘æƒ³è¿™ä¸ªä¸»è¦æ˜¯æƒ³é—®ä¸¤è€…çš„lossçš„ä¸åŒä»¥åŠè¾“å‡ºçš„ä¸åŒï¼Œä¸€ä¸ªæ˜¯æ¦‚ç‡è¾“å‡ºä¸€ä¸ªæ˜¯scoreï¼‰
- both solves classification; SVM can solve regression too.
- loss
- output

LRå¤§éƒ¨åˆ†é¢ç»é›†ä¸­åœ¨loglosså’Œregularization

Support Vector Machine (SVM)
- a classifier defined by a separating hyperplane(decision boundary), where same distance from the boundary point of both classes
- kernel tricks: a method of using a linear classifier to solve a non-linear problem; kernel transforms linearly inseparable data to linearly separable ones by mapping them into higher-dimensional space.

- SVM is based on geometrical properties of data; logistic regression is statistical.
- SVM less vulnarable to overfitting

- output SVM as a probability: å¦‚ä½•è®©SVMè¾“å‡ºåˆ†ç±»æ¦‚ç‡ 
    - sigmoid-fitting
    - SVM + Logistic Regression blending

## 4.6. å…¶ä»–æ¨¡å‹

Principal component analysis (PCA)
- create new uncorrelated variables that successively maximize variance by solving an eigenvalue/eigenvector problem.
- reduce the dimensionality of dataset, increase interpretability while minimize information loss
- pros: no need of prior; reduce overfitting (by reduce #variables in the dataset); visualizable
- cons: data standardization is a prerequisite; information loss

t-SNE
- t-Distributed Stochastic Neighbor Embedding is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data.
- t-SNE gives you a feel or intuition of how the data is arranged in a high-dimensional space.

UMAP
- Uniform Manifold Approximation and Projection is a novel manifold learning technique for dimension reduction. 
- UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data.


Kernel Method/Tricks
- best known member is SVM
- enable to operate in a high-dimensional, implicit feature space without computing the coordinates of the data in that space.
- why use kernel:
- pros: often computationally cheaper than explicit computation of the coordinates; 
- cons: 
- Radial basis function kernel exp( - sqrt((x1 - x2)^2) / 2 * Ïƒ^2)
- polynomial kernel: (x1^T * x2 + c) ** d


Explain KNN
!æ‰€æœ‰æ¨¡å‹çš„pros and cons ï¼ˆæœ€é«˜é¢‘çš„ä¸€ä¸ªé—®é¢˜ï¼‰


# 5. Data Processing
Data Augmentation
- synthesize new data by modifying existing data
- resize, horizontal/vertical flip, rotate, add noise, deform

imbalanced data (long-tail data, skewed class data)
- change performance metric
    - Kappa/Cohenâ€™s kappa: Classification accuracy normalized by the imbalance of the classes in the data.
- resampling to even up the class
- collect more data, generate synthetic samples
    - Synthetic Minority Oversampling Technique: SMOTE uses KNN to create new instances
- bias the model to pay more attention to minority class by penalty
- balanced bagging classifier
- train the model with major classes, fine tune with minor.

problem with high-dim classification
* Curse of Dimensionality: number of training examples we need to cover the space densely, exponential in the dimensionality of the problem; we will never get enough examples; also computational burden
    - manual feature selection; PCA; multidimensional scaling; locally linear embedding
* N grows exponentially with D
* solution
    - LDA-type methods: Naive Bayes, Nearest Shrunken Centroid, Sparse LDA, regularized LDA
    - penalized logistic regression
    - large-margin methods (SVM)
    - classification tree (random forest)
    - boosting


missing data
- 60% observations missing, better discard it 
- mean, median, mode of the existing observations (dont use in time-series characteristics)
- multiple imputation and combine multiple analysis to produce an overall result
- mean among K Nearest Neighbors

Missing Time-Series Data
- last observation carried forward, next observation carried backward
- linear interpolation
- seasonal adjustment with linear interpolation


Feature Selection (~dimensionality reduction):
- wrapper: create many models with different subsets of features and select those features that result in the best performing model according to a performance metric. 
    - unconcerned with variable types, computationally expensive
- filter: evaluate the relationship between each input feature and the target variable (importance of feature)
- intrinsic: algorithms perform feature selection automatically as part of learning the model (penalized regression, LASSO; random forest)
- unsupervised: correlation


how to capture feature interaction


# 6. implementation & derivation
```python
class FullyConnected:
    def __init__(self, inp_size, out_size):
        self.W = np.random.rand(inp_size, out_size)
        self.b = np.random.rand(1, out_size)

    def forward(self, x):
        self.inp = x
        return np.dot(self.inp, self.W) + self.b

    def backward(self, out_error, learning_rate):
        self.W -= learning_rate * (np.dot(self.inp.T, out_error))
        self.b -= learning_rate * out_error


class Convolution:
    def __init__(self):
        # we have [H0, W0, C0], C1, kernel_size, stride, pad, dilation.
        # H1 = floor((H0 - k + 2 * p) / s) + 1

    def forward(self,)

```

```python
# KNN --------------------------------------
def KNN(input, data, k=5):
    # input is (N, ), data is (B, N)
    input = np.expand_dims(input, axis=1)
    # Euclidean distance
    metrics = np.sqrt(np.sum(np.subtract(data, input.T) ** 2, axis=1))
    topk = np.argsort(metrics)[: k]
    return topk  # classfication by vote, regression by mean


# KMeans Clustering ------------------------
def get_centriods(data, assignment):
    # data is [B, N], assignment is a dict
    centriods = []
    for k in assignment:
        data_in_k = assignment[k]
        centriods.append(np.mean(np.array(data_in_k), axis=0))
    return centriods  # [K, N]


def assign(data, centriods):
    assignments = dict()
    for i in range(data.shape[0]):
        point = data[i, :]
        assignto = np.argsort(np.sum(np.subtract(point, centriods) ** 2, axis=1))[0]
        assignments[assignto] =  assignments.get(assignto, []) + [point]

    return assignments


def get_MSE(data, centriods, assignment):
    MSE = 0
    for k, vs in assignment.items():
        center = centriods[k]
        for v in vs:
            MSE += np.sum((v - center) ** 2)
    return MSE ** 0.5 / data.shape[0] 


def initialize_centriods(data, k):
    return data[:k, :]

def initialize_centriods_random(data, k):
    return np.random.rand(k, data.shape[1])


def KMeans(data, k=3):

    threshold=0.01
    early_stopping = 10

    centriods = initialize_centriods_random(data, k)
    print(centriods)
    assignment = assign(data, centriods)

    count = 0
    while get_MSE(data, centriods, assignment) > threshold \
        and count < early_stopping:

        centriods = get_centriods(data, assignment)
        assignment = assign(data, centriods)
        print(centriods[0])
        count += 1
```
æ‰‹å†™softmaxçš„backpropagation
ç»™ä¸€ä¸ªLSTM networkçš„ç»“æ„è¦ä½ è®¡ç®—how many parameters


# 7. NLP/RNNç›¸å…³

Variants of RNN
- LSTM, GRU, end-to-end network, memory network


LSTMçš„å…¬å¼æ˜¯ä»€ä¹ˆ
why use RNN/LSTM

LSTM
- special RNN, but capable of leanring long-term dependencies
- nature of remembering information for a long periods of time
- forget gate, input gate, output gate

limitation of RNN
- gradient vanishing (use LSTM)
- training is difficult

GRU
- Gated Recurrent Unitsï¼Œæ˜¯å¾ªç¯ç¥ç»ç½‘ç»œçš„ä¸€ç§
- GRUåªæœ‰ä¸¤ä¸ªé—¨ï¼ˆupdateå’Œresetï¼‰ï¼ŒLSTMæœ‰ä¸‰ä¸ªé—¨ï¼ˆforgetï¼Œinputï¼Œoutputï¼‰ï¼ŒGRUç›´æ¥å°†hidden stateä¼ ç»™ä¸‹ä¸€ä¸ªå•å…ƒï¼Œè€ŒLSTMç”¨memory cellæŠŠhidden stateåŒ…è£…èµ·æ¥


Attention
* self attention mechanism allows the inputs to interact with each other, find out who they should pay more attention to; outputs are aggregates of interactions and attention scores
    - why attention

```python
# x is an embedding vector of inputs
querys = x @ w_query
keys = x @ w_key
values = x @ w_value

attention_scores = softmax(querys @ keys.T, dim=-1)  # softmax for normalization
# scaled attention_scores for back-propagation gradient stability
# attention_scores = softmax(querys @ keys.T / sqrt(d), dim=-1)        
# where d is the dimension of query vector
weighted_values = values * attention_scores
output = weighted_values.sum(dim=0)
```
* cross attention

Transformer
- attention to Transformer
* Multi-Head?
* encoder/decoder part?


Language Modelçš„åŸç†ï¼ŒN-Gram Model
Whatâ€™s CBOW and skip-gram?
ä»€ä¹ˆæ˜¯Word2Vecï¼Œ loss functionæ˜¯ä»€ä¹ˆï¼Œ negative samplingæ˜¯ä»€ä¹ˆ

# 8. CNN/CVç›¸å…³

pooling
- reduce the dim of featuremaps, i.e., number of parameters to learn/computations.
- pooling layer summarises the features present in a region of the feature map generated by a convolution layer.
- maxpooling

conv layeræ˜¯ä»€ä¹ˆ, ä¸ºä»€ä¹ˆç”¨conv layï¼Œ

ä»€ä¹ˆæ˜¯equivariant to translation, invariant to translation

1x1 filter: used to reduce the dimension of depth channel, while not look at anything around itself.

ResNet ä»€ä¹ˆæ˜¯skip connection
- ResNet34: conv7x7 + pool + 16 ResNet Blocks (some skip connection omitted) + avgpool + fc1000
- ResNet block: ReLU( conv3x3( ReLU( conv3x3(x) ) + x ) )
- ResNeXt: 
    - cardinality/group: a hyperparameter indicating the number of independent paths, to provide a new way of adjusting the model capacity
    - 'split-transform-merge' is usually done by pointwise grouped convlayer, which divides input into groups of feature maps and perform convolution respectively, their outputs are depth-concentrated and then fed into1 1x1 convlayer.


CNNçš„å…³é”®å±‚æœ‰:
â‘ è¾“å…¥å±‚ï¼Œå¯¹æ•°æ®å»å‡å€¼ï¼Œåšdata augmentationç­‰å·¥ä½œ
â‘¡å·ç§¯å±‚ï¼Œå±€éƒ¨å…³è”æŠ½å–feature
â‘¢æ¿€æ´»å±‚ï¼Œéçº¿æ€§å˜åŒ–
â‘£æ± åŒ–å±‚ï¼Œä¸‹é‡‡æ ·
â‘¤å…¨è¿æ¥å±‚ï¼Œå¢åŠ æ¨¡å‹éçº¿æ€§
â‘¥é«˜é€Ÿé€šé“ï¼Œå¿«é€Ÿè¿æ¥
â‘¦BNå±‚ï¼Œç¼“è§£æ¢¯åº¦å¼¥æ•£


CNNçš„ç‰¹ç‚¹ä»¥åŠä¼˜åŠ¿
- CNNä½¿ç”¨èŒƒå›´æ˜¯å…·æœ‰å±€éƒ¨ç©ºé—´ç›¸å…³æ€§çš„æ•°æ®ï¼Œæ¯”å¦‚å›¾åƒï¼Œè‡ªç„¶è¯­è¨€ï¼Œè¯­éŸ³
- å±€éƒ¨è¿æ¥ï¼šå¯ä»¥æå–å±€éƒ¨ç‰¹å¾ã€‚
- æƒå€¼å…±äº«ï¼šå‡å°‘å‚æ•°æ•°é‡ï¼Œå› æ­¤é™ä½è®­ç»ƒéš¾åº¦ï¼ˆç©ºé—´ã€æ—¶é—´æ¶ˆè€—éƒ½å°‘äº†ï¼‰ã€‚å¯ä»¥å®Œå…¨å…±äº«ï¼Œä¹Ÿå¯ä»¥å±€éƒ¨å…±äº«ï¼ˆæ¯”å¦‚å¯¹äººè„¸ï¼Œçœ¼ç›é¼»å­å˜´ç”±äºä½ç½®å’Œæ ·å¼ç›¸å¯¹å›ºå®šï¼Œå¯ä»¥ç”¨å’Œè„¸éƒ¨ä¸ä¸€æ ·çš„å·ç§¯æ ¸ï¼‰
- é™ç»´ï¼šé€šè¿‡æ± åŒ–æˆ–å·ç§¯strideå®ç°
- å¤šå±‚æ¬¡ç»“æ„ï¼šå°†ä½å±‚æ¬¡çš„å±€éƒ¨ç‰¹å¾ç»„åˆæˆä¸ºè¾ƒé«˜å±‚æ¬¡çš„ç‰¹å¾ã€‚ä¸åŒå±‚çº§çš„ç‰¹å¾å¯ä»¥å¯¹åº”ä¸åŒä»»åŠ¡


# 9. VAE, GANs
Auto-Encoder
- used to learn a compressed form of given data
- data denoising, dimensionality reduction, image reconstruction, image colorization
  
GAN
- generator + discriminator



# 10. Loss Function & Optimization

1. regression
MAE (L1)
MSE (L2)
smooth L1 loss
Huber loss

2. classification
0-1 loss
hinge loss
cross entropy


3. detection, segmentation
focal loss
triplet loss
contrastive loss
center loss
IOU loss

ArcFace loss


optimizer
1. Stochastic Gradient Descent (SGD)
2. SGD with Momentum (= mass Ã— velocity, cannot instantaneously change direction)
3. SGD with Nesterov momentum
4. AdaGrad
5. RMSProp
6. Adam(adaptive moments)


# 11. Detection
FPN RPN

1-stage detection
2-stage detection

rcnn fast-rcnn faster-rcnn
ä¼ ç»Ÿçš„detectionä¸»æµæ–¹æ³•æ˜¯DPM(Deformable parts models)ï¼Œ åœ¨VOC2007ä¸Šèƒ½åˆ°43%çš„mAPï¼Œè™½ç„¶DPMå’ŒCNNçœ‹èµ·æ¥å·®åˆ«å¾ˆå¤§ï¼Œä½†RBGå¤§ç¥è¯´â€œDeformable Part Models are Convolutional Neural Networksâ€ï¼ˆhttp://arxiv.org/abs/1409.5403ï¼‰

CNNæµè¡Œä¹‹åï¼ŒSzegedyåšè¿‡å°†detectioné—®é¢˜ä½œä¸ºå›å½’é—®é¢˜çš„å°è¯•ï¼ˆDeep Neural Networks for Object Detectionï¼‰ï¼Œä½†æ˜¯æ•ˆæœå·®å¼ºäººæ„ï¼Œåœ¨VOC2007ä¸ŠmAPåªæœ‰30.5%ã€‚æ—¢ç„¶å›å½’æ–¹æ³•æ•ˆæœä¸å¥½ï¼Œè€ŒCNNåœ¨åˆ†ç±»é—®é¢˜ä¸Šæ•ˆæœå¾ˆå¥½ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆä¸æŠŠdetectioné—®é¢˜è½¬åŒ–ä¸ºåˆ†ç±»é—®é¢˜å‘¢ï¼Ÿ

RBGçš„RCNNä½¿ç”¨region proposalï¼ˆå…·ä½“ç”¨çš„æ˜¯Selective Search Koen van de Sande: Segmentation as Selective Search for Object Recognitionï¼‰æ¥å¾—åˆ°æœ‰å¯èƒ½å¾—åˆ°æ˜¯objectçš„è‹¥å¹²ï¼ˆå¤§æ¦‚10^3é‡çº§ï¼‰å›¾åƒå±€éƒ¨åŒºåŸŸï¼Œç„¶åæŠŠè¿™äº›åŒºåŸŸåˆ†åˆ«è¾“å…¥åˆ°CNNä¸­ï¼Œå¾—åˆ°åŒºåŸŸçš„featureï¼Œå†åœ¨featureä¸ŠåŠ ä¸Šåˆ†ç±»å™¨ï¼Œåˆ¤æ–­featureå¯¹åº”çš„åŒºåŸŸæ˜¯å±äºå…·ä½“æŸç±»objectè¿˜æ˜¯èƒŒæ™¯ã€‚å½“ç„¶ï¼ŒRBGè¿˜ç”¨äº†åŒºåŸŸå¯¹åº”çš„featureåšäº†é’ˆå¯¹boundingboxçš„å›å½’ï¼Œç”¨æ¥ä¿®æ­£é¢„æµ‹çš„boundingboxçš„ä½ç½®ã€‚

RCNNåœ¨VOC2007ä¸Šçš„mAPæ˜¯58%å·¦å³ã€‚RCNNå­˜åœ¨ç€é‡å¤è®¡ç®—çš„é—®é¢˜ï¼ˆproposalçš„regionæœ‰å‡ åƒä¸ªï¼Œå¤šæ•°éƒ½æ˜¯äº’ç›¸é‡å ï¼Œé‡å éƒ¨åˆ†ä¼šè¢«å¤šæ¬¡é‡å¤æå–featureï¼‰ï¼Œäºæ˜¯RBGå€Ÿé‰´Kaiming Heçš„SPP-netçš„æ€è·¯å•æªåŒ¹é©¬æå‡ºäº†Fast-RCNNï¼Œè·ŸRCNNæœ€å¤§åŒºåˆ«å°±æ˜¯Fast-RCNNå°†proposalçš„regionæ˜ å°„åˆ°CNNçš„æœ€åä¸€å±‚conv layerçš„feature mapä¸Šï¼Œè¿™æ ·ä¸€å¼ å›¾ç‰‡åªéœ€è¦æå–ä¸€æ¬¡featureï¼Œå¤§å¤§æé«˜äº†é€Ÿåº¦ï¼Œä¹Ÿç”±äºæµç¨‹çš„æ•´åˆä»¥åŠå…¶ä»–åŸå› ï¼Œåœ¨VOC2007ä¸Šçš„mAPä¹Ÿæé«˜åˆ°äº†68%ã€‚

æ¢ç´¢æ˜¯æ— æ­¢å¢ƒçš„ã€‚Fast-RCNNçš„é€Ÿåº¦ç“¶é¢ˆåœ¨Region proposalä¸Šï¼Œäºæ˜¯RBGå’ŒKaiming Heä¸€å¸®äººå°†Region proposalä¹Ÿäº¤ç»™CNNæ¥åšï¼Œæå‡ºäº†Faster-RCNNã€‚Fater-RCNNä¸­çš„region proposal netwrokå®è´¨æ˜¯ä¸€ä¸ªFast-RCNNï¼Œè¿™ä¸ªFast-RCNNè¾“å…¥çš„region proposalçš„æ˜¯å›ºå®šçš„ï¼ˆæŠŠä¸€å¼ å›¾ç‰‡åˆ’åˆ†æˆn*nä¸ªåŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸç»™å‡º9ä¸ªä¸åŒratioå’Œscaleçš„proposalï¼‰ï¼Œè¾“å‡ºçš„æ˜¯å¯¹è¾“å…¥çš„å›ºå®šproposalæ˜¯å±äºèƒŒæ™¯è¿˜æ˜¯å‰æ™¯çš„åˆ¤æ–­å’Œå¯¹é½ä½ç½®çš„ä¿®æ­£ï¼ˆregressionï¼‰ã€‚Region proposal networkçš„è¾“å‡ºå†è¾“å…¥ç¬¬äºŒä¸ªFast-RCNNåšæ›´ç²¾ç»†çš„åˆ†ç±»å’ŒBoundingboxçš„ä½ç½®ä¿®æ­£ã€‚

Fater-RCNNé€Ÿåº¦æ›´å¿«äº†ï¼Œè€Œä¸”ç”¨VGG netä½œä¸ºfeature extractoræ—¶åœ¨VOC2007ä¸ŠmAPèƒ½åˆ°73%ã€‚ä¸ªäººè§‰å¾—åˆ¶çº¦RCNNæ¡†æ¶å†…çš„æ–¹æ³•ç²¾åº¦æå‡çš„ç“¶é¢ˆæ˜¯å°†dectectioné—®é¢˜è½¬åŒ–æˆäº†å¯¹å›¾ç‰‡å±€éƒ¨åŒºåŸŸçš„åˆ†ç±»é—®é¢˜åï¼Œä¸èƒ½å……åˆ†åˆ©ç”¨å›¾ç‰‡å±€éƒ¨objectåœ¨æ•´ä¸ªå›¾ç‰‡ä¸­çš„contextä¿¡æ¯ã€‚

å¯èƒ½RBGä¹Ÿæ„è¯†åˆ°äº†è¿™ä¸€ç‚¹ï¼Œæ‰€ä»¥ä»–æœ€æ–°çš„ä¸€ç¯‡æ–‡ç« YOLOï¼ˆhttp://arxiv.org/abs/1506.02640ï¼‰åˆå›åˆ°äº†regressionçš„æ–¹æ³•ä¸‹ï¼Œè¿™ä¸ªæ–¹æ³•æ•ˆæœå¾ˆå¥½ï¼Œåœ¨VOC2007ä¸ŠmAPèƒ½åˆ°63.4%ï¼Œè€Œä¸”é€Ÿåº¦éå¸¸å¿«ï¼Œèƒ½è¾¾åˆ°å¯¹è§†é¢‘çš„å®æ—¶å¤„ç†ï¼ˆæ²¹ç®¡è§†é¢‘ï¼šhttps://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebgï¼‰ï¼Œè™½ç„¶ä¸å¦‚Fast-RCNNï¼Œä½†æ˜¯æ¯”ä¼ ç»Ÿçš„å®æ—¶æ–¹æ³•ç²¾åº¦æå‡äº†å¤ªå¤šï¼Œè€Œä¸”æˆ‘è§‰å¾—è¿˜æœ‰æå‡ç©ºé—´ã€‚


Bounding-Box regression
- bounding box (x, y, w, h), x, y are center coordinates
- BB regression å…ˆå¹³ç§», åå°ºåº¦ç¼©æ”¾


Selective Search
- (1)ä¸€ç§è¿‡åˆ†å‰²æ‰‹æ®µï¼Œå°†å›¾åƒåˆ†å‰²æˆå°åŒºåŸŸ (1k~2k ä¸ª)
- (2)æŸ¥çœ‹ç°æœ‰å°åŒºåŸŸï¼ŒæŒ‰ç…§åˆå¹¶è§„åˆ™åˆå¹¶å¯èƒ½æ€§æœ€é«˜çš„ç›¸é‚»ä¸¤ä¸ªåŒºåŸŸã€‚é‡å¤ç›´åˆ°æ•´å¼ å›¾åƒåˆå¹¶æˆä¸€ä¸ªåŒºåŸŸä½ç½®
    ä¼˜å…ˆåˆå¹¶ä»¥ä¸‹å››ç§åŒºåŸŸ:
    â‘ é¢œè‰²ï¼ˆé¢œè‰²ç›´æ–¹å›¾ï¼‰ç›¸è¿‘çš„
    â‘¡çº¹ç†ï¼ˆæ¢¯åº¦ç›´æ–¹å›¾ï¼‰ç›¸è¿‘çš„
    â‘¢åˆå¹¶åæ€»é¢ç§¯å°çš„: ä¿è¯åˆå¹¶æ“ä½œçš„å°ºåº¦è¾ƒä¸ºå‡åŒ€ï¼Œé¿å…ä¸€ä¸ªå¤§åŒºåŸŸé™†ç»­â€œåƒæ‰â€å…¶ä»–å°åŒºåŸŸ 
    â‘£åˆå¹¶å, æ€»é¢ç§¯åœ¨å…¶BBOXä¸­æ‰€å æ¯”ä¾‹å¤§çš„ï¼š ä¿è¯åˆå¹¶åå½¢çŠ¶è§„åˆ™
- (3)è¾“å‡ºæ‰€æœ‰æ›¾ç»å­˜åœ¨è¿‡çš„åŒºåŸŸï¼Œæ‰€è°“å€™é€‰åŒºåŸŸ

éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰
- æŠ‘åˆ¶ä¸æ˜¯æå¤§å€¼çš„å…ƒç´ ï¼Œæœç´¢å±€éƒ¨çš„æå¤§å€¼ã€‚è¿™ä¸ªå±€éƒ¨ä»£è¡¨çš„æ˜¯ä¸€ä¸ªé‚»åŸŸï¼Œé‚»åŸŸæœ‰ä¸¤ä¸ªå‚æ•°å¯å˜ï¼Œä¸€æ˜¯é‚»åŸŸçš„ç»´æ•°ï¼ŒäºŒæ˜¯é‚»åŸŸçš„å¤§å°
- åœ¨ç›®æ ‡æ£€æµ‹ä¸­ç”¨äºæå–åˆ†æ•°æœ€é«˜çš„çª—å£
    (1)ä»æœ€å¤§æ¦‚ç‡çŸ©å½¢æ¡†Få¼€å§‹ï¼Œåˆ†åˆ«åˆ¤æ–­A~Eä¸Fçš„é‡å åº¦IOUæ˜¯å¦å¤§äºæŸä¸ªè®¾å®šçš„é˜ˆå€¼
    (2)å‡è®¾Bã€Dä¸Fçš„é‡å åº¦è¶…è¿‡é˜ˆå€¼ï¼Œé‚£ä¹ˆå°±æ‰”æ‰Bã€Dï¼›å¹¶æ ‡è®°ç¬¬ä¸€ä¸ªçŸ©å½¢æ¡†Fï¼Œæ˜¯æˆ‘ä»¬ä¿ç•™ä¸‹æ¥çš„
    (3)ä»å‰©ä¸‹çš„çŸ©å½¢æ¡†Aã€Cã€Eä¸­ï¼Œé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„Eï¼Œç„¶ååˆ¤æ–­Eä¸Aã€Cçš„é‡å åº¦ï¼Œé‡å åº¦å¤§äºä¸€å®šçš„é˜ˆå€¼ï¼Œå°±æ‰”æ‰ï¼›å¹¶æ ‡è®°Eæ˜¯æˆ‘ä»¬ä¿ç•™ä¸‹æ¥çš„ç¬¬äºŒä¸ªçŸ©å½¢æ¡†

anchor
- ä½¿ç”¨ä¸€ä¸ª3*3çš„å·ç§¯æ ¸ï¼Œåœ¨æœ€åä¸€ä¸ªfeature mapä¸Šæ»‘åŠ¨ï¼Œå½“æ»‘åŠ¨åˆ°ç‰¹å¾å›¾çš„æŸä¸€ä¸ªä½ç½®æ—¶ï¼Œä»¥å½“å‰æ»‘åŠ¨çª—å£ä¸­å¿ƒä¸ºä¸­å¿ƒæ˜ å°„å›åŸå›¾çš„ä¸€ä¸ªåŒºåŸŸ(æ³¨æ„ feature map ä¸Šçš„ä¸€ä¸ªç‚¹æ˜¯å¯ä»¥æ˜ å°„åˆ°åŸå›¾çš„ä¸€ä¸ªåŒºåŸŸçš„ï¼Œç›¸å½“äºæ„Ÿå—é‡èµ·çš„ä½œç”¨)ï¼Œä»¥åŸå›¾ä¸Šè¿™ä¸ªåŒºåŸŸçš„ä¸­å¿ƒå¯¹åº”ä¸€ä¸ªå°ºåº¦å’Œé•¿å®½æ¯”ï¼Œå°±æ˜¯ä¸€ä¸ªanchoräº†
- fast rcnn ä½¿ç”¨3ç§å°ºåº¦å’Œ3ç§é•¿å®½æ¯”ï¼ˆ1:1ï¼›1:2ï¼›2:1ï¼‰ï¼Œåˆ™åœ¨æ¯ä¸€ä¸ªæ»‘åŠ¨ä½ç½®å°±æœ‰ 3*3 = 9 ä¸ªanchor



IOU:
2 cases:
- 2 points: x1 = 1 and x2 = 3, the distance is x2 - x1 = 2
- 2 pixels of index: i1 = 1 and i2 = 3, the segment from pixel i1 to i2 contains 3 pixels, l = i2 - i1 + 1

```python
def IOU(box1, box2): #[x1, y1, x2, y2], [x3, y3, x4, y4]

    # check if intersect
    if box1[2] < box2[0] or box1[0] > box2[2]: # check x, x2 < x3 | x1 > x4
        return 0
    if box1[3] < box2[1] or box1[1] > box2[3]: # check y
        return 0

    # find intersection
    x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])
    x2, y2 = min(box1[2], box2[2]), min(box1[3], box2[3])
    i_area = (x2 - x1) * (y2 - y1)

    # find sum of areas
    b1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    b2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # iou = intersection / (sum_of_areas - intersection)
    return i_area / (b1_area + b2_area - i_area) 


import torch
import torchvision.ops.boxes as bops

box1 = torch.tensor([box1], dtype=torch.float)
box2 = torch.tensor([box2], dtype=torch.float)
iou = bops.box_iou(box1, box2)


# Vectorized IOU
def vec_IOU(boxes1, boxes2): # both [N, 4]
    assert(boxes1[:, 2:] > boxes1[:, :2]).all()
    assert(boxes2[:, 2:] > boxes2[:, :2]).all()

    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])

    # Intersection
    top_left = np.maximum(boxes1[:, :2], boxes2[:, :2])      # [[x, y]]
    bottom_right = np.minimum(boxes1[:, 2:], boxes2[:, 2:])  # [[x, y]]
    wh = bottom_right - top_left
    
    # clip: if boxes not overlap then make it zero
    intersection = wh[:, 0].clip(0) * wh[:, 1].clip(0)

    # Union 
    union = area1 + area2 - intersection

    return intersection / union
```

Â  Â  Â  Â 
# 12. é¡¹ç›®ç»éªŒç±»
è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨ç°å®ä¸­ä¸work

æ·±åº¦å­¦ä¹ ä¸­æœ‰ä»€ä¹ˆåŠ å¿«æ”¶æ•›/é™ä½è®­ç»ƒéš¾åº¦çš„æ–¹æ³•?
- ç“¶é¢ˆç»“æ„ bottleneck
- æ®‹å·® residual block
- å­¦ä¹ ç‡ã€æ­¥é•¿ã€åŠ¨é‡
- ä¼˜åŒ–æ–¹æ³•
- é¢„è®­ç»ƒ

Loss Inf/NaN:
- learning rate too high
- gradient exploding (add gradient clipping)
- divide by 0
- input contains NaN
- NaN running_mean or running_var in BatchNorm

ç”Ÿäº§å’Œå¼€å‘æ—¶å€™dataå‘ç”Ÿäº†ä¸€äº›shiftåº”è¯¥å¦‚ä½•detectå’Œè¡¥æ•‘

Data Shift: 
- covariate shift (shift in independent variables/input features)
    - change in the distribution of the input variables present in the training and test set. 
    - can be addressed by batch normalization
    - e.g train a cat classifier on the set of images containing white cats and other non-cat images, then it is applied on test of black cats.
- prior probability shift(shift in the target variable)
- concept shift(shift in the relationship between the independent and the target variable)

Training with limited annotation
- data annotation; data augumentation
- pre-training with synthenic data or some geometric constraints (homography transformation), like superpoint
- pretrained backbone/model, fine tune
- self-supervised learning; unsupervised learning
- distillation

å‡è®¾æœ‰ä¸ªmodelè¦æ”¾productionäº†ä½†æ˜¯å‘ç°online one important feature missingä¸èƒ½é‡æ–°train model ä½ æ€ä¹ˆåŠ



# 13. å…³äºå‡†å¤‡è€ƒML æ¦‚å¿µçš„é¢è¯•çš„ä¸€äº›å»ºè®®
1. å¦‚æœä½ ç®€å†ä¸Šæåˆ°äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯·ç¡®ä¿ä½ å¯¹è¿™ä¸ªæ¨¡å‹æœ‰ç€æ·±å…¥å…¨é¢çš„äº†è§£ ï¼ˆæ¯”å¦‚å¾ˆå¤šäººå¯èƒ½ç®€å†é‡Œéƒ½æåˆ°äº†XgBoostï¼Œä½†æ˜¯å¯èƒ½äº†è§£å¹¶ä¸å…¨é¢ï¼‰
ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ç®€å†ä¸Šæåˆ°äº†Graph Convolutional NNï¼Œ æˆ‘é¢è¯•çš„æ—¶å€™å°±è¢«è¦æ±‚ä¸ç”¨åŒ…æ‰‹å†™ä¸€ä¸ªç®€å•çš„GCNã€‚
2. å¦‚æœjob descriptionä¸Šæåˆ°äº†æŸäº›æ¨¡å‹ï¼Œæœ€å¥½å¯¹è¿™äº›æ¨¡å‹ä¹Ÿæ¯”è¾ƒç†Ÿæ‚‰ã€‚
3. å¯¹ä½ è¿™ä¸ªç»„çš„domainçš„ç›¸å…³æ¨¡å‹è¦ç†Ÿæ‚‰ã€‚
æ¯”å¦‚ï¼Œä½ é¢ä¸€ä¸ªæ˜ç¡®åšNLPçš„ç»„ï¼Œé‚£ä¹ˆä¸Šè¿°é¢ç»å°±è¿‡äºåŸºç¡€äº†ã€‚
ä½ æˆ–è®¸è¿˜è¦çŸ¥é“ What is BERTï¼Œ explain the model architectureï¼›what is Transformer modelï¼Œ explain the model architectureï¼›Transformer/BERT æ¯”LSTMå¥½åœ¨å“ªï¼›difference between self attention and traditional attention mechanismï¼›æˆ–è®¸ä½ è¿˜è¦çŸ¥é“ä¸€äº›ç®€å•çš„åšdistillçš„æ–¹æ³•..æˆ–è®¸æ ¹æ®ç»„çš„æ–¹å‘ä½ è¿˜è¦çŸ¥é“ASR, æˆ–è€…Chat botç­‰ç­‰çš„æ–¹å‘çš„ä¸€äº›widely usedçš„æ¨¡å‹æˆ–è€…æ–¹æ³•ã€‚
æ¯”å¦‚ä½ é¢ä¸€ä¸ªCTRçš„ç»„ï¼Œæˆ–è®¸å¯èƒ½ä½ å¤§æ¦‚è‡³å°‘è¦ç¨å¾®äº†è§£ä¸‹wide-and-deep
æ¯”å¦‚ä½ é¢ä¸€ä¸ªCV-segmentçš„ç»„ï¼Œä½ æˆ–è®¸å¯èƒ½è¦äº†è§£DeepMaskï¼ŒU-Net...ç­‰ç­‰..
ä½ åº”è¯¥ä¸ä¸€å®šéœ€è¦çŸ¥é“æœ€SOTAçš„æ¨¡å‹ï¼Œä½†æ˜¯çŸ¥é“é‚£äº›æœ€å¹¿ä¸ºè¿ç”¨çš„æ¨¡å‹æˆ–è®¸å¯èƒ½æ˜¯å¿…è¦çš„ã€‚




p-value
- probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct


Gradient descent: an optimization algorithm used to find the values of parameters of a function that minimizes a cost function. It is to be used when the parameters cannot be found analytically.

Â Â 3ï¼‰Gradient descent è§£é‡ŠåŸç†ï¼Œä»€ä¹ˆæ˜¯ mini batch GD, stachastic GD, Adam
Â Â 4ï¼‰NN é‡Œé¢ gradient descentæ€ä¹ˆè®¡ç®—ï¼Œæ˜¯convexçš„å—ï¼Œèƒ½ä¿è¯æœ€ä¼˜è§£å—ï¼Œï¼ˆä¸èƒ½ä¿è¯ï¼‰æ€ä¹ˆè§£å†³
Â Â 5ï¼‰Regression ç”¨ä»€ä¹ˆloss? Classification ç”¨ä»€ä¹ˆlossï¼Œ å¤šåˆ†ç±»å‘¢ï¼Ÿåˆ†ç±»çš„lossæ˜¯convexçš„å—
Â Â 7ï¼‰Random forest hyperparameter æ€ä¹ˆé€‰
Â Â 8ï¼‰Validation set éƒ½ç”¨æ¥å¹²å˜›


MLEã€MAPå’Œè´å¶æ–¯ä¼°è®¡
MLEï¼ŒMAPï¼ŒEM å’Œ point estimation ä¹‹é—´çš„å…³ç³»æ˜¯æ€æ ·çš„ï¼Ÿ


IOUã€NMS
SVMçš„ä¼˜ç¼ºç‚¹
éšæœºæ£®æ—çš„è®­ç»ƒè¿‡ç¨‹
ä¼˜åŒ–æ–¹æ³•SGDã€Batch GDã€Adadeltaã€Momentumå¯¹è¶…å‚æ•°çš„æ•æ„Ÿç¨‹åº¦
CNNä¸­feature mapç»´åº¦è®¡ç®—ã€å›¾ä¸­æ¯ä¸€ä¸ªç‰¹å¾ç‚¹åœ¨åŸå›¾çš„æ„Ÿå—é‡å¤§å°
Segmatch


æ¨¡å‹å‹ç¼©ä¸åŠ é€Ÿ mobilenet v1ã€mobilenet v2ã€shufflenet
MobileNet V2

ç›®æ ‡åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ï¼ˆone stageã€two stageï¼‰ï¼ŒYOLOä¸‰ä»£çš„å‘å±•ï¼Œå°ç›®æ ‡æ£€æµ‹
Mask-RCNN vs YOLO


# 14. Pytorch Example
```python
import torch
import math

# Create Tensors to hold input and outputs.
x = torch.linspace(-math.pi, math.pi, 2000)
y = torch.sin(x)

# Prepare the input tensor (x, x^2, x^3).
p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)

# Use the nn package to define our model and loss function.
model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),
    torch.nn.Flatten(0, 1)
)
loss_fn = torch.nn.MSELoss(reduction='sum')

# Use the optim package to define an Optimizer that will update the weights of
# the model for us. Here we will use RMSprop; the optim package contains many other
# optimization algorithms. The first argument to the RMSprop constructor tells the
# optimizer which Tensors it should update.
learning_rate = 1e-3
optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
for t in range(2000):
    # Forward pass: compute predicted y by passing x to the model.
    y_pred = model(xx)

    # Compute and print loss.
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # Before the backward pass, use the optimizer object to zero all of the
    # gradients for the variables it will update (which are the learnable
    # weights of the model). This is because by default, gradients are
    # accumulated in buffers (i.e, not overwritten) whenever .backward()
    # is called. Checkout docs of torch.autograd.backward for more details.
    optimizer.zero_grad()

    # Backward pass: compute gradient of the loss with respect to model parameters
    loss.backward()

    # Calling the step function on an Optimizer makes an update to its parameters
    optimizer.step()
```

Pytorch Inference
```python
torch.set_grad_enabled(False)

model.eval()

with torch.no_grad():
    ...
```


# ToDo List
SVM 
- hyperplane dimension NxM?
- computational trick


Faster RCNN vs MaskRCNN
