

# MLåŸºç¡€æ¦‚å¿µç±»
overfitting/underfiting:
- underfitting means large training error, large generalization error; overfitting means small training error, large generalization error
- overfitting means training loss is small and testing loss is large, small regularization
- overfit means complex model, underfit means over-simplified model.
- overfitting: model is tailored to a particular dataset and is unable to generalise to other datasets

bias/variance trade off
- A set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of parameter estimates across samples, and vice versa.
- linear/logistic regression: underfittingâ€”high bias, overfittingâ€”high variance
- high bias (model miss relevant relations between features and target outputâ€”underfitting)
- high variance (model modelled random noise in the training dataâ€”overfitting)

Bias Variance Decomposition: Error = Bias ** 2 + Variance + Irreducible Error


How to overcome/prevent Overfitting:
1. Regularization 
  - L1 regularization (lasso penalty) favours few non-zero coefficients   Î»âˆ‘âˆ£Î¸âˆ£
  - L2 regularization (ridge/Tikhonov penalty) favours small coefficients	 Î»âˆ‘Î¸2
  - Mixed L1/L2 regularization (elastic net)  Î»1âˆ‘âˆ£Î¸âˆ£+Î»2âˆ‘Î¸2
  - L^p regularization (penalty on parameters)
2. early stopping: interrupt training when its performance on the validation set starts dropping
3. Max-norm: for each neuron the weight Î¸ of the incoming connections are constrained such that  ğ„Î¸ğ„2â‰¤r  (clipping)
  - compute ğ„Î¸ğ„2 after each training step and clip Î¸ if needed Î¸=Î¸r/ğ„Î¸ğ„2
  - max-norm hyperparameter r increases the amount of regularization
4. dropout: at every training step, every neuron (input/hidden) has a probability of p of being temporarily dropped out
   â‰ˆ an ensemble learning method
5. data augmentation: variation of input data; artificially boosting the size of training set
 by rotate, shift(translate), resize(scale), flip(reflect), or add different lighting conditions or noise that can be learnt
6. Stochastic Gradient Descent

Generative/Discrimitive:
- Generative models model the (actual) distribution of each class.
- given training data, Generative models generate new samples from same distribution, learn Pmodel(x)â‰ˆPdata(x)
- Discriminative models learn the (hard or soft) decision boundary between the classes. 

Give a set of ground truths and 2 models, how do you be confident that one model is better than another?

## Reguarlization
Occamâ€™s Razor: among competing hypotheses, the simplest is the best. 

L1 regularization (lasso penalty) 
- favours few non-zero coefficients   Î»âˆ‘âˆ£Î¸âˆ£
- prior: Laplace distribution with mean zero and a scale parameter a function of Î»

L2 regularization (ridge/Tikhonov penalty) 
- favours small coefficients	 Î»âˆ‘Î¸2
- prior: a Gaussian with mean zero and standard deviation a function of Î»
- Optimizing with respect to L2 norm corresponds to maximizing the (log-)likelihood of the data under a Gaussian prior. 

Lasso/Ridgeçš„æ¨å¯¼

Why L1 sparse: L2 penalties in some sense discourage sparsity by yielding diminishing returns as elements are moved closer to zero

ä¸ºä»€ä¹ˆregularization works
ä¸ºä»€ä¹ˆregularizationç”¨L1 L2ï¼Œè€Œä¸æ˜¯L3, L4..

## Metric
Confusion Matrix
- A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned)

Precision (Positive Predictive Value)= true positive / (true positive + false positive)
- measure of exactness
Recall (True Positive rate, sensitivity) = true positive / (true positive + false negative)
- measure of completeness

F/F1-Score: weighted average of precision and recall 2 * (P * R) / (P + R)

True Negative Rate/Specificity
False Positive rate = FP / (FP + TN)
Accuracy

ROC curve (receiver operating characteristic)
- showing the performance of a classification model at all classification thresholds
- x-axis: FP rate, y-axis: TP rate  (TPR vs. FPR at different classification thresholds)


AUC (Area Under the ROC Curve)
- an aggregate measure of performance across all possible classification thresholds
- A model whose predictions are 100% wrong has an AUC of 0; one whose predictions are 100% correct has an AUC of 1.
- scale invariant. It measures how well predictions are ranked, rather than their absolute values.
- classification-threshold invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.

precision and recall, trade-off

label ä¸å¹³è¡¡æ—¶ç”¨ä»€ä¹ˆmetric
åˆ†ç±»é—®é¢˜è¯¥é€‰ç”¨ä»€ä¹ˆmetricï¼Œand why
confusion matrix
AUCçš„è§£é‡Š (the probability of ranking a randomly selected positive sample higher blablabla....)
Log-lossæ˜¯ä»€ä¹ˆï¼Œä»€ä¹ˆæ—¶å€™ç”¨logloss
è¿˜æœ‰ä¸€äº›å’Œåœºæ™¯æ¯”è¾ƒç›¸å…³çš„é—®é¢˜ï¼Œæ¯”å¦‚ranking designçš„æ—¶å€™ç”¨ä»€ä¹ˆmetricï¼Œæ¨èçš„æ—¶å€™ç”¨ä»€ä¹ˆ

## Loss & Optomization
ç”¨MSEåšlossçš„Logistic Rregressionæ˜¯convex problemå—
è§£é‡Šå¹¶å†™å‡ºMSEçš„å…¬å¼, ä»€ä¹ˆæ—¶å€™ç”¨åˆ°MSE?
Linear Regressionæœ€å°äºŒä¹˜æ³•å’ŒMLEå…³ç³»
ä»€ä¹ˆæ˜¯relative entropy/crossentropy

Kullback-Leiber Divergence
- intuition: 
- a measure of difference of 2 probability distributions (how close 2 distributions are)


Logistic Regressionçš„lossæ˜¯ä»€ä¹ˆ
Logistic Regressionçš„ Loss æ¨å¯¼
SVMçš„lossæ˜¯ä»€ä¹ˆ
Multiclass Logistic Regressionç„¶åé—®äº†ä¸€ä¸ªä¸ºä»€ä¹ˆç”¨cross entropyåšcost function
Decision Tree split nodeçš„æ—¶å€™ä¼˜åŒ–ç›®æ ‡æ˜¯å•¥


# DLåŸºç¡€æ¦‚å¿µç±»

bias term in DNN: shift the activation function

DNNä¸ºä»€ä¹ˆè¦æœ‰bias term, bias termçš„intuitionæ˜¯ä»€ä¹ˆ
ä»€ä¹ˆæ˜¯Back Propagation

Epoch
- one pass through the whole training set
- If there are N training samples and use B batch size, then it takes N/B iterations to complete on epoch

Gradient Vanishing & Exploding
- Vanishing:
    - activation function squishes a large input space into a small one (e.g., sigmoid 0 to 1); large change in the input results in small change in the output --> derivative small
    - use other activations, like ReLU (not cause a small derivative)
    - residual network
    - batch normalization (normalize the input)
    - use more complex RNN
- Exploding: gradient becomes very large, preventing the algorithm from converging
    - gradient clipping (clip by value, clip by norm)

Initialize all weights to be 0?
- constant initialization leads to same gradient propagated back everywhere (symmetric)

DNNå’ŒLogistic Regressionçš„åŒºåˆ«; ä¸ºä»€ä¹ˆDNNçš„æ‹Ÿåˆèƒ½åŠ›æ¯”Logistic Regressionå¼º? 
- Non-linearity introduced by activation functions

Hyperparameter tuning in DL: random search, grid search

prevent overfitting in DL: dropout, early stopping, data augumentation
- ä»€ä¹ˆæ˜¯Dropoutï¼Œwhy it worksï¼Œdropoutçš„æµç¨‹æ˜¯ä»€ä¹ˆ (è®­ç»ƒå’Œæµ‹è¯•æ—¶çš„åŒºåˆ«)
- ä»€ä¹ˆæ˜¯Batch Norm, why it works, BNçš„æµç¨‹æ˜¯ä»€ä¹ˆ (è®­ç»ƒå’Œæµ‹è¯•æ—¶çš„åŒºåˆ«)

Activation Functions
1. Logistic/Sigmoid: 1 / (1 + exp(-x))
    - 0.0 ~ 1.0, f(z) saturates at 0 or 1, i.e., derivative becomes almost 0/megligible.
2. Hyperbolic Tangent tanh: (e^x â€“ e^-x) / (e^x + e^-x)
    - -1.0 ~ +1.0, f(z) saturates at -1 or 1
    - 
3. Rectified Linear Unit 
    - discontinuity at 0 but has sub-gradient
    - outperform the sigmoid function
    - gradient will not vanish when z is very large, but gradient becomes 0 when z becomes negative, which may still lead to vanishing gradient problem
    - leaky relu: f(z)=0.01z, z<0; z, z>0
    - Parametric ReLU    f(z)=az, z<0; z, z>0    (learning parameter a in training)
    - Exponential Linear unit  f(z)=a(e^zï¹£1), z<0; z, zâ‰¥0

Softmax: e^x / sum(e^x)

ä¸ºä»€ä¹ˆéœ€è¦non-linear activation functions

Different optimizers (SGD, RMSprop, Momentum, Adagradï¼ŒAdam) çš„åŒºåˆ«

Batch vs  SGD
- Stochastic gradient descent: approximate the gradient using current sample
- Mini-Batch: use a small batch of b samples
- Small batch size:
    - stalls at less accurate result, lower iteration cost
    - gradient may become sensitive to a single training sample
- Large batch size:
    - stalls at more accurate result, higher iteration cost
    - computation will become expensive and use more memory on GPU.

learning rate/step size
1. small step size: 
    - slower initial convergence, stalls at more accurate result
    - optimization may be slow
2. large step size: 
    - faster initial convergence, stalls at less accurate result
    - With a high learning rate, the optimization is faster but may bounce chaotically and not arrive at good local minima
	- With a very high learning rate, the optimization may diverge

Problem of Plateau, saddle point

When transfer learning makes sense?
- Transfer learning only works in deep learning if the model features learned from the first task are general.


# MLæ¨¡å‹ç±»
## Regression:
four principal assumptions for Linear Regression:
1. Linearity and additivity
2. statistical independence
3. homoscedasticity
4. normality

what will happen when we have correlated variables, how to solve
explain regression coefficient

what is the relationship between minimizing squared errorÂ and maximizing the likelihood

How could you minimize the inter-correlation between variables with Linear Regression?
if the relationship between y and x is no linear, can linear regression solve that

why use interaction variables


## Clustering and EM:
K-means clustering (explain the algorithm in detail; whether it will converge, æ”¶æ•›åˆ°global or local optimums;Â Â how to stop)

EMç®—æ³•æ˜¯ä»€ä¹ˆ
GMMæ˜¯ä»€ä¹ˆï¼Œå’ŒKmeansçš„å…³ç³»


## Decision Tree
How regression/classification DT split nodes?
How to prevent overfitting in DT?
How to do regularization in DT?


## Ensemble Learning
Bagging: (Bootstrap Aggregating): reduce model variance through averaging
- Bootstraping (train separate weak learners on each Bootstrap sample)
- Aggregating results: classification uses majority votes; regression used averaging.

Boosting:
- build weak learners in serial/sequential
- but adaptively re-weight training data prior training each new weak learner, in order to get a higher weight to previously misclassified examples

Random Forest: homogenous ensemble learning of decision trees
- bult using Bagging (each decision tree as a parallel estimator)
- pros: a whitebox model (easy to understand, interpretable, visualizable); not require normalization; 
- cons: need uncorrelated decision trees (Bootstrapping plays a key role in creating uncorrelated decision trees.)

gradient boosted decision trees (GBDT):
- built using Boosting (decision trees connected in series)
- to minimize the error of previous tree
- Decision trees in GBDT are not fit to the entire dataset. Each tree fits to the residuals from the previous one. 
- As a result, the overall accuracy and robustness of the model gradually increase.
- pros: do not need bootstrapping, no correlated trees; no need to create subsamples from the dataset

will random forest help reduce bias or variance/why random forest can help reduce variance


## Generative Model
å’ŒDiscrimitiveæ¨¡å‹æ¯”èµ·æ¥ï¼ŒGenerative æ›´å®¹æ˜“overfittingè¿˜æ˜¯underfitting


NaÃ¯ve Bayesçš„åŸç†ï¼ŒåŸºç¡€å‡è®¾æ˜¯ä»€ä¹ˆ


LDA/QDAæ˜¯ä»€ä¹ˆï¼Œå‡è®¾æ˜¯ä»€ä¹ˆ
- Latent Dirichlet allocation

Linear/normal discriminant analysis (LDA) vs Quadratic discriminant analysis (QDA)
- conditional probability density function P(x|y=0) and P(x|y=1) are normal distributions N(Î¼0, Î£0) and N(Î¼1, Î£1)
- Bayes optimal solution is to predict points as being from the second class if the log of the likelihood ratios is bigger than some threshold T: (x - Î¼0)Î£0(x - Î¼0) + ln|Î£0| - (x - Î¼1)Î£1(x - Î¼1) - ln|Î£1| > T  (QDA)
- LDA: assume Î£0=Î£1, dot(w, x) > c where w = pow(Î£, -1) * (Î¼1 - Î¼0) and c = dot(w, (Î¼1 + Î¼0) / 2)
    - the criterion of an input x being in a class y is purely a function of the linear combination of the known observations.


## Logistic Regression
logistic regression vs svmï¼ˆæˆ‘æƒ³è¿™ä¸ªä¸»è¦æ˜¯æƒ³é—®ä¸¤è€…çš„lossçš„ä¸åŒä»¥åŠè¾“å‡ºçš„ä¸åŒï¼Œä¸€ä¸ªæ˜¯æ¦‚ç‡è¾“å‡ºä¸€ä¸ªæ˜¯scoreï¼‰
- both solves classification; SVM can solve regression too.
- loss
- output

LRå¤§éƒ¨åˆ†é¢ç»é›†ä¸­åœ¨loglosså’Œregularizationï¼Œ

SVM
- a classifier defined by a separating hyperplane(decision boundary), where same distance from the boundary point of both classes
- kernel tricks: a method of using a linear classifier to solve a non-linear problem; kernel transforms linearly inseparable data to linearly separable ones by mapping them into higher-dimensional space.

- SVM is based on geometrical properties of data; logistic regression is statistical.
- SVM less vulnarable to overfitting

- output SVM as a probability: å¦‚ä½•è®©SVMè¾“å‡ºåˆ†ç±»æ¦‚ç‡ 
    - sigmoid-fitting
    - SVM + Logistic Regression blending

## å…¶ä»–æ¨¡å‹

Principal component analysis (PCA)
- create new uncorrelated variables that successively maximize variance by solving an eigenvalue/eigenvector problem.
- reduce the dimensionality of dataset, increase interpretability while minimize information loss
- pros: no need of prior; reduce overfitting (by reduce #variables in the dataset); visualizable
- cons: data standardization is a prerequisite; information loss


Kernel Method/Tricks
- best known member is SVM
- enable to operate in a high-dimensional, implicit feature space without computing the coordinates of the data in that space.
- why use kernel:
- pros: often computationally cheaper than explicit computation of the coordinates; 
- cons: 
- Radial basis function kernel exp( - sqrt((x1 - x2)^2) / 2 * Ïƒ^2)
- polynomial kernel: (x1^T * x2 + c) ** d


Explain KNN
!æ‰€æœ‰æ¨¡å‹çš„pros and cons ï¼ˆæœ€é«˜é¢‘çš„ä¸€ä¸ªé—®é¢˜ï¼‰


# Data Processing
imbalanced data
- change performance metric
    - Kappa/Cohenâ€™s kappa: Classification accuracy normalized by the imbalance of the classes in the data.
- resampling to even up the class
- collect more data, generate synthetic samples
    - Synthetic Minority Oversampling Technique: SMOTE uses KNN to create new instances
- bias the model to pay more attention to minority class by penalty
- balanced bagging classifier

problem with high-dim classification
- Curse of Dimensionality: number of training examples we need to cover the space densely, exponential in the dimensionality of the problem; we will never get enough examples; also computational burden
- N grows exponentially with D
- solution
    - LDA-type methods: Naive Bayes, Nearest Shrunken Centroid, Sparse LDA, regularized LDA
    - penalized logistic regression
    - large-margin methods (SVM)
    - classification tree (random forest)
    - boosting


missing data
- 60% observations missing, better discard it 
- mean, median, mode of the existing observations (dont use in time-series characteristics)
- multiple imputation and combine multiple analysis to produce an overall result
- mean among K Nearest Neighbors

Missing Time-Series Data
- last observation carried forward, next observation carried backward
- linear interpolation
- seasonal adjustment with linear interpolation


Feature Selection (~dimensionality reduction):
- wrapper: create many models with different subsets of features and select those features that result in the best performing model according to a performance metric. 
    - unconcerned with variable types, computationally expensive
- filter: evaluate the relationship between each input feature and the target variable (importance of feature)
- intrinsic: algorithms perform feature selection automatically as part of learning the model (penalized regression, LASSO; random forest)
- unsupervised: correlation


how to capture feature interaction


# implementation & derivation
```python
class FullyConnected:
    def __init__(self, inp_size, out_size):
        self.W = np.random.rand(inp_size, out_size)
        self.b = np.random.rand(1, out_size)

    def forward(self, x):
        self.inp = x
        return np.dot(self.inp, self.W) + self.b

    def backward(self, out_error, learning_rate):
        self.W -= learning_rate * (np.dot(self.inp.T, out_error))
        self.b -= learning_rate * out_error


class Convolution:
    def __init__(self):
        # we have [H0, W0, C0], C1, kernel_size, stride, pad, dilation.
        # H1 = floor((H0 - k + 2 * p) / s) + 1

    def forward(self,)

```

```python
# KNN --------------------------------------
def KNN(input, data, k=5):
    # input is (N, ), data is (B, N)
    input = np.expand_dims(input, axis=1)
    # Euclidean distance
    metrics = np.sqrt(np.sum(np.subtract(data, input.T) ** 2, axis=1))
    topk = np.argsort(metrics)[: k]
    return topk  # classfication by vote, regression by mean


# KMeans Clustering ------------------------
def get_centriods(data, assignment):
    # data is [B, N], assignment is a dict
    centriods = []
    for k in assignment:
        data_in_k = assignment[k]
        centriods.append(np.mean(np.array(data_in_k), axis=0))
    return centriods  # [K, N]


def assign(data, centriods):
    assignments = dict()
    for i in range(data.shape[0]):
        point = data[i, :]
        assignto = np.argsort(np.sum(np.subtract(point, centriods) ** 2, axis=1))[0]
        assignments[assignto] =  assignments.get(assignto, []) + [point]

    return assignments


def get_MSE(data, centriods, assignment):
    MSE = 0
    for k, vs in assignment.items():
        center = centriods[k]
        for v in vs:
            MSE += np.sum((v - center) ** 2)
    return MSE ** 0.5 / data.shape[0] 


def initialize_centriods(data, k):
    return data[:k, :]

def initialize_centriods_random(data, k):
    return np.random.rand(k, data.shape[1])


def KMeans(data, k=3):

    threshold=0.01
    early_stopping = 10

    centriods = initialize_centriods_random(data, k)
    print(centriods)
    assignment = assign(data, centriods)

    count = 0
    while get_MSE(data, centriods, assignment) > threshold \
        and count < early_stopping:

        centriods = get_centriods(data, assignment)
        assignment = assign(data, centriods)
        print(centriods[0])
        count += 1
```
æ‰‹å†™softmaxçš„backpropagation
ç»™ä¸€ä¸ªLSTM networkçš„ç»“æ„è¦ä½ è®¡ç®—how many parameters

Â  Â  Â  Â 
# é¡¹ç›®ç»éªŒç±»
è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨ç°å®ä¸­ä¸work

Loss Inf/NaN:
- learning rate too high
- gradient exploding (add gradient clipping)
- divide by 0
- input contains NaN
- NaN running_mean or running_var in BatchNorm

ç”Ÿäº§å’Œå¼€å‘æ—¶å€™dataå‘ç”Ÿäº†ä¸€äº›shiftåº”è¯¥å¦‚ä½•detectå’Œè¡¥æ•‘

Data Shift: 
- covariate shift (shift in independent variables/input features)
    - change in the distribution of the input variables present in the training and test set. 
    - can be addressed by batch normalization
    - e.g train a cat classifier on the set of images containing white cats and other non-cat images, then it is applied on test of black cats.
- prior probability shift(shift in the target variable)
- concept shift(shift in the relationship between the independent and the target variable)

Training with limited annotation
- data annotation; data augumentation
- pre-training with synthenic data or some geometric constraints (homography transformation), like superpoint
- pretrained backbone/model, fine tune
- self-supervised learning; unsupervised learning
- distillation

å‡è®¾æœ‰ä¸ªmodelè¦æ”¾productionäº†ä½†æ˜¯å‘ç°online one important feature missingä¸èƒ½é‡æ–°train model ä½ æ€ä¹ˆåŠ

# NLP/RNNç›¸å…³
LSTMçš„å…¬å¼æ˜¯ä»€ä¹ˆ
why use RNN/LSTM

LSTM
- special RNN, but capable of leanring long-term dependencies
- nature of remembering information for a long periods of time
- forget gate, input gate, output gate

limitation of RNN
- gradient vanishing (use LSTM)
- training is difficult

attention
- self attention mechanism allows the inputs to interact with each other, find out who they should pay more attention to; outputs are aggregates of interactions and attention scores
- why attention

```python
querys = x @ w_query
keys = x @ w_key
values = x @ w_value

attention_scores = softmax(querys @ keys.T, dim=-1)
weighted_values = values * attention_scores
output = weighted_values.sum(dim=0)
```
- cross attention
- attention to Transformer

Language Modelçš„åŸç†ï¼ŒN-Gram Model
Whatâ€™s CBOW and skip-gram?
ä»€ä¹ˆæ˜¯Word2Vecï¼Œ loss functionæ˜¯ä»€ä¹ˆï¼Œ negative samplingæ˜¯ä»€ä¹ˆ

# CNN/CVç›¸å…³

pooling
- reduce the dim of featuremaps, i.e., number of parameters to learn/computations.
- pooling layer summarises the features present in a region of the feature map generated by a convolution layer.
- maxpooling

conv layeræ˜¯ä»€ä¹ˆ, ä¸ºä»€ä¹ˆç”¨conv layï¼Œ

ä»€ä¹ˆæ˜¯equivariant to translation, invariant to translation

1x1 filter: used to reduce the dimension of depth channel, while not look at anything around itself.

ResNet ä»€ä¹ˆæ˜¯skip connection
- ResNet34: conv7x7 + pool + 16 ResNet Blocks (some skip connection omitted) + avgpool + fc1000
- ResNet block: ReLU( conv3x3( ReLU( conv3x3(x) ) + x ) )
- ResNeXt: 
    - cardinality/group: a hyperparameter indicating the number of independent paths, to provide a new way of adjusting the model capacity
    - 'split-transform-merge' is usually done by pointwise grouped convlayer, which divides input into groups of feature maps and perform convolution respectively, their outputs are depth-concentrated and then fed into1 1x1 convlayer.

FPN RPN

1-stage detection
2-stage detection

# Loss Function

1. regression
MAE (L1)
MSE (L2)
smooth L1 loss
Huber loss

2. classification
0-1 loss
hinge loss
cross entropy


3. detection, segmentation
focal loss
triplet loss
contrastive loss
center loss
IOU loss

ArcFace loss

# å…³äºå‡†å¤‡è€ƒML æ¦‚å¿µçš„é¢è¯•çš„ä¸€äº›å»ºè®®
1. å¦‚æœä½ ç®€å†ä¸Šæåˆ°äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯·ç¡®ä¿ä½ å¯¹è¿™ä¸ªæ¨¡å‹æœ‰ç€æ·±å…¥å…¨é¢çš„äº†è§£ ï¼ˆæ¯”å¦‚å¾ˆå¤šäººå¯èƒ½ç®€å†é‡Œéƒ½æåˆ°äº†XgBoostï¼Œä½†æ˜¯å¯èƒ½äº†è§£å¹¶ä¸å…¨é¢ï¼‰
ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ç®€å†ä¸Šæåˆ°äº†Graph Convolutional NNï¼Œ æˆ‘é¢è¯•çš„æ—¶å€™å°±è¢«è¦æ±‚ä¸ç”¨åŒ…æ‰‹å†™ä¸€ä¸ªç®€å•çš„GCNã€‚
2. å¦‚æœjob descriptionä¸Šæåˆ°äº†æŸäº›æ¨¡å‹ï¼Œæœ€å¥½å¯¹è¿™äº›æ¨¡å‹ä¹Ÿæ¯”è¾ƒç†Ÿæ‚‰ã€‚
3. å¯¹ä½ è¿™ä¸ªç»„çš„domainçš„ç›¸å…³æ¨¡å‹è¦ç†Ÿæ‚‰ã€‚
æ¯”å¦‚ï¼Œä½ é¢ä¸€ä¸ªæ˜ç¡®åšNLPçš„ç»„ï¼Œé‚£ä¹ˆä¸Šè¿°é¢ç»å°±è¿‡äºåŸºç¡€äº†ã€‚
ä½ æˆ–è®¸è¿˜è¦çŸ¥é“ What is BERTï¼Œ explain the model architectureï¼›what is Transformer modelï¼Œ explain the model architectureï¼›Transformer/BERT æ¯”LSTMå¥½åœ¨å“ªï¼›difference between self attention and traditional attention mechanismï¼›æˆ–è®¸ä½ è¿˜è¦çŸ¥é“ä¸€äº›ç®€å•çš„åšdistillçš„æ–¹æ³•..æˆ–è®¸æ ¹æ®ç»„çš„æ–¹å‘ä½ è¿˜è¦çŸ¥é“ASR, æˆ–è€…Chat botç­‰ç­‰çš„æ–¹å‘çš„ä¸€äº›widely usedçš„æ¨¡å‹æˆ–è€…æ–¹æ³•ã€‚
æ¯”å¦‚ä½ é¢ä¸€ä¸ªCTRçš„ç»„ï¼Œæˆ–è®¸å¯èƒ½ä½ å¤§æ¦‚è‡³å°‘è¦ç¨å¾®äº†è§£ä¸‹wide-and-deep
æ¯”å¦‚ä½ é¢ä¸€ä¸ªCV-segmentçš„ç»„ï¼Œä½ æˆ–è®¸å¯èƒ½è¦äº†è§£DeepMaskï¼ŒU-Net...ç­‰ç­‰..
ä½ åº”è¯¥ä¸ä¸€å®šéœ€è¦çŸ¥é“æœ€SOTAçš„æ¨¡å‹ï¼Œä½†æ˜¯çŸ¥é“é‚£äº›æœ€å¹¿ä¸ºè¿ç”¨çš„æ¨¡å‹æˆ–è®¸å¯èƒ½æ˜¯å¿…è¦çš„ã€‚




p-value
- probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct
- 

Â Â 3ï¼‰Gradient descent è§£é‡ŠåŸç†ï¼Œä»€ä¹ˆæ˜¯ mini batch GD, stachastic GD, Adam
Â Â 4ï¼‰NN é‡Œé¢ gradient descentæ€ä¹ˆè®¡ç®—ï¼Œæ˜¯convexçš„å—ï¼Œèƒ½ä¿è¯æœ€ä¼˜è§£å—ï¼Œï¼ˆä¸èƒ½ä¿è¯ï¼‰æ€ä¹ˆè§£å†³
Â Â 5ï¼‰Regression ç”¨ä»€ä¹ˆloss? Classification ç”¨ä»€ä¹ˆlossï¼Œ å¤šåˆ†ç±»å‘¢ï¼Ÿåˆ†ç±»çš„lossæ˜¯convexçš„å—
Â Â 7ï¼‰Random forest hyperparameter æ€ä¹ˆé€‰
Â Â 8ï¼‰Validation set éƒ½ç”¨æ¥å¹²å˜›